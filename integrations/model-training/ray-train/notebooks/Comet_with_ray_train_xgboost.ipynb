{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.comet.ml/img/notebook_logo.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Comet](https://www.comet.com/site/products/ml-experiment-tracking/?utm_campaign=ray_train&utm_medium=colab) is an MLOps Platform that is designed to help Data Scientists and Teams build better models faster! Comet provides tooling to track, Explain, Manage, and Monitor your models in a single place! It works with Jupyter Notebooks and Scripts and most importantly it's 100% free to get started!\n",
    "\n",
    "[Ray Train](https://docs.ray.io/en/latest/train/train.html) abstracts away the complexity of setting up a distributed training system.\n",
    "\n",
    "Instrument your runs with Comet to start managing experiments, create dataset versions and track hyperparameters for faster and easier reproducibility and collaboration.\n",
    "\n",
    "[Find more information about our integration with Ray Train](https://www.comet.ml/docs/v2/integrations/ml-frameworks/ray/)\n",
    "\n",
    "Get a preview for what's to come. Check out a completed experiment created from this notebook [here](https://www.comet.com/examples/comet-example-ray-train-xgboost/43c968fda9e74260996f8cafb5b9f32c).\n",
    "\n",
    "This example is based on the [following Ray Train XGBoost example](https://docs.ray.io/en/latest/train/distributed-xgboost-lightgbm.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZYchV5RWwdv5"
   },
   "source": [
    "# Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DJnmqphuY2eI"
   },
   "outputs": [],
   "source": [
    "%pip install -U comet_ml \"ray[air]>=2.1.0\" xgboost_ray \"pandas!=2.2.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "crOcPHobwhGL"
   },
   "source": [
    "# Initialize Comet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HNQRM0U3caiY"
   },
   "outputs": [],
   "source": [
    "import comet_ml\n",
    "import comet_ml.integration.ray\n",
    "\n",
    "comet_ml.init(project_name=\"comet-example-ray-train-xgboost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cgqwGSwtzVWD"
   },
   "source": [
    "# Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e-5rRYaUw5AF"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import ray\n",
    "from ray.air.config import RunConfig, ScalingConfig\n",
    "from ray.train import Result\n",
    "from ray.train.xgboost import XGBoostTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data.\n",
    "dataset = ray.data.read_csv(\"s3://anonymous@air-example-data/breast_cancer.csv\")\n",
    "# Split data into train and validation.\n",
    "train_dataset, valid_dataset = dataset.train_test_split(\n",
    "    test_size=0.3, shuffle=True, seed=536\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the function that schedule the distributed job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgboost(\n",
    "    num_workers: int = 2, use_gpu: bool = False, num_boost_round: int = 20\n",
    ") -> Result:\n",
    "    config = {}\n",
    "    callback = comet_ml.integration.ray.CometTrainLoggerCallback(config)\n",
    "\n",
    "    trainer = XGBoostTrainer(\n",
    "        scaling_config=ScalingConfig(\n",
    "            # Number of workers to use for data parallelism.\n",
    "            num_workers=num_workers,\n",
    "            # Whether to use GPU acceleration. Set to True to schedule GPU workers.\n",
    "            use_gpu=use_gpu,\n",
    "        ),\n",
    "        label_column=\"target\",\n",
    "        num_boost_round=num_boost_round,\n",
    "        params={\n",
    "            # XGBoost specific params (see the `xgboost.train` API reference)\n",
    "            \"objective\": \"binary:logistic\",\n",
    "            # uncomment this and set `use_gpu=True` to use GPU for training\n",
    "            # \"tree_method\": \"gpu_hist\",\n",
    "            \"eval_metric\": [\"logloss\", \"error\"],\n",
    "            # Make the build reproducible\n",
    "            \"random_state\": 536,\n",
    "        },\n",
    "        datasets={\"train\": train_dataset, \"valid\": valid_dataset},\n",
    "        run_config=RunConfig(callbacks=[callback]),\n",
    "    )\n",
    "    result = trainer.fit()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model\n",
    "\n",
    "Ray will wait indefinitely if we request more num_workers that the available resources, the code below ensure we never request more CPU than available locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ideal_num_workers = 2\n",
    "\n",
    "available_local_cpu_count = os.cpu_count() - 1\n",
    "num_workers = min(ideal_num_workers, available_local_cpu_count)\n",
    "\n",
    "if num_workers < 1:\n",
    "    num_workers = 1\n",
    "\n",
    "train_xgboost(num_workers, use_gpu=False, num_boost_round=10)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
