{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://github.com/comet-ml/opik/blob/main/apps/opik-documentation/documentation/static/img/opik-logo.svg?raw=true\" width=\"200\" height=\"100\" alt=\"Opik Logo\">\n",
        "\n",
        "# Comet Assistant: RAG with Opik\n",
        "\n",
        "The below example walks through the process of building a simple RAG application with OpenAI and langchain, and evaluating the application with Opik.\n",
        "\n",
        "The concepts covered in this tutorial include:\n",
        "\n",
        "1. Setting up a simple vector store and RAG pipeline with langchain\n",
        "2. Defining an assistant application using this RAG pipeline and the OpenAI API\n",
        "3. Creating a dataset of questions for evaluation in Opik\n",
        "4. Automating the evaluation of the application on the dataset using Opik metrics"
      ],
      "metadata": {
        "id": "n242nv-d3vfL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating an account on Comet.com\n",
        "\n",
        "[Comet](https://www.comet.com/site?from=llm&utm_source=opik&utm_medium=colab&utm_content=langchain&utm_campaign=opik) provides a hosted version of the Opik platform, [simply create an account](https://www.comet.com/signup?from=llm&utm_source=opik&utm_medium=colab&utm_content=langchain&utm_campaign=opik) and grab you API Key.\n",
        "\n",
        "> You can also run the Opik platform locally, see the [installation guide](https://www.comet.com/docs/opik/self-host/overview/?from=llm&utm_source=opik&utm_medium=colab&utm_content=langchain&utm_campaign=opik) for more information."
      ],
      "metadata": {
        "id": "G7d_ldIRqSbD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade --quiet opik openai"
      ],
      "metadata": {
        "id": "wybtUdOwmNX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from opik import Opik, track\n",
        "import opik\n",
        "from opik.evaluation import evaluate\n",
        "from opik.evaluation.metrics import AnswerRelevance, LevenshteinRatio\n",
        "import openai"
      ],
      "metadata": {
        "id": "TRPCwfT9mlV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opik.configure(use_local=False)"
      ],
      "metadata": {
        "id": "jTuv6r2KmaDU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d40f80f-7168-4c41-ec87-b5af3aaf641d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Opik is already configured. You can check the settings by viewing the config file at /root/.opik.config\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup the vector store for RAG"
      ],
      "metadata": {
        "id": "p2pAd1tbqUss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install --quiet langsmith langchain-community langchain chromadb tiktoken langchain_openai"
      ],
      "metadata": {
        "id": "CusPXnn9bWD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")"
      ],
      "metadata": {
        "id": "JFi5qEgbkIAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Set up Vector Store and Retriever.**\n",
        "\n",
        "The below code sets up a vector store using [Chroma](https://www.trychroma.com/). Here we are loading Zoox FAQ documentation."
      ],
      "metadata": {
        "id": "nBsNkp9RqvKB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.documents import Document\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Load from text file\n",
        "file_path = \"/content/zoox_faq.txt\"\n",
        "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Wrap into Document (list of one document)\n",
        "docs = [Document(page_content=text, metadata={\"source\": file_path})]\n",
        "\n",
        "# Split\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=50)\n",
        "splits = splitter.split_documents(docs)\n",
        "\n",
        "# Embed\n",
        "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
        "\n",
        "# Index\n",
        "retriever = vectorstore.as_retriever()"
      ],
      "metadata": {
        "id": "yn_1QkpWbkz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Define RAG Application\n",
        "The below code defines our LLM application. In this case, we create a Comet bot that 1) retrieves relevant context from our vector store based on the input 2) inputs the relevant question + user question into OpenAI to retrieve a response.\n",
        "\n",
        "In order to ensure that the OpenAI API calls are being tracked, we will be using the `track_openai` function from the Opik library. We will also use the `track` decorator to ensure each step of the application is tracked."
      ],
      "metadata": {
        "id": "VkN5_LsRq8pn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "from opik.integrations.openai import track_openai\n",
        "from opik import track, opik_context, Attachment\n",
        "from pathlib import Path\n",
        "import base64\n",
        "from PIL import Image\n",
        "\n",
        "PROJECT_NAME = \"Zoox User Assistant\"\n",
        "\n",
        "def encode_image_to_base64(image_path: str) -> str:\n",
        "    with open(image_path, \"rb\") as image_file:\n",
        "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
        "\n",
        "def detect_mime(image_path: Path) -> str:\n",
        "    with Image.open(image_path) as img:\n",
        "        return f\"image/{img.format.lower()}\"\n",
        "\n",
        "def make_image_url(image_path: Path):\n",
        "  image_data = encode_image_to_base64(image_path)\n",
        "  mime_type = detect_mime(image_path)\n",
        "  return {\n",
        "    \"url\": f\"data:{mime_type};base64,{image_data}\"\n",
        "  }\n",
        "\n",
        "\n",
        "\n",
        "class CometImageBot:\n",
        "    def __init__(self, retriever, model: str = \"gpt-4o\"):\n",
        "        self._retriever = retriever\n",
        "        self._client = track_openai(openai.Client(), project_name=PROJECT_NAME)\n",
        "        self._model = model\n",
        "\n",
        "    @track(project_name=PROJECT_NAME)\n",
        "    def retrieve_docs(self, question: str):\n",
        "        return self._retriever.invoke(question)\n",
        "\n",
        "    @track(project_name=PROJECT_NAME)\n",
        "    def get_answer(self, question: str, image_url: str, system: str = \"You are an assistant that answers questions about images using both visual and text context.\"):\n",
        "        docs_retrieved = self.retrieve_docs(question)\n",
        "\n",
        "\n",
        "        # Create vision prompt\n",
        "        messages = [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": system + \"\\nUse the following docs to answer the question:\\n\\n\" + str(docs_retrieved),\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"text\", \"text\": question},\n",
        "                    {\n",
        "                        \"type\": \"image_url\",\n",
        "                        \"image_url\": image_url,\n",
        "                    },\n",
        "                ],\n",
        "            },\n",
        "        ]\n",
        "\n",
        "        response = self._client.chat.completions.create(\n",
        "            model=self._model,\n",
        "            messages=messages,\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"response\": response.choices[0].message.content,\n",
        "            \"context\": [str(doc) for doc in docs_retrieved],\n",
        "        }"
      ],
      "metadata": {
        "id": "PVYnJcfGkRq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing the Bot and the Retriever with one system prompt"
      ],
      "metadata": {
        "id": "vURdawlnxOI-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bot = CometImageBot(retriever)\n",
        "image_url = make_image_url(Path(\"seatbelt.png\"))\n",
        "\n",
        "bot.get_answer(\n",
        "    question=\"how do I buckle my seatbelt\",\n",
        "    image_url=image_url\n",
        ")"
      ],
      "metadata": {
        "id": "xxNz6l9JkvR-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c038a4b-a491-43d5-d7c6-178d0c77bc28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Started logging traces to the \"Zoox User Assistant\" project at https://www.comet.com/opik/api/v1/session/redirect/projects/?trace_id=01988561-5712-7295-add2-2088af71b3c2&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'response': 'To buckle your seatbelt:\\n\\n1. Pull the seatbelt across your body.\\n2. Insert the metal clip into the buckle until you hear a click.\\n3. Adjust the strap so it fits snugly across your lap and shoulder.\\n\\nEnsure the belt is not twisted and lies flat against your body for safety.',\n",
              " 'context': [\"page_content='Do I have to wear a seat belt?\\nYes. The robotaxi won’t move until all riders are wearing their seatbelts. Please stay seated and buckled at all times, even when the robotaxi isn’t moving.' metadata={'source': '/content/zoox_faq.txt'}\",\n",
              "  \"page_content='row between the side seat panel and the window. Simultaneously, pull up the handle with your left hand and push the door closest to the handle with your right hand.' metadata={'source': '/content/zoox_faq.txt'}\",\n",
              "  \"page_content='If you need to open the doors in an emergency and the above methods do not work, you can use the Emergency Door Handle on the far left of each seat row between the side seat panel and the window.' metadata={'source': '/content/zoox_faq.txt'}\",\n",
              "  \"page_content='Riders under 4’1” in height should use a high-back booster seat\\nSee Zoox Inc.’s Terms of Service, opens in a new tab for the full Code of Conduct' metadata={'source': '/content/zoox_faq.txt'}\"]}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating a Dataset: Comet Questions\n",
        "\n",
        "Below we define a standard set of questions that we would like to evaluate the assistant on."
      ],
      "metadata": {
        "id": "5hZmnIk_sRNh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_items = [\n",
        "  {\n",
        "    \"question\": \"What are the rules about wearing seatbelt and how do I use it\",\n",
        "    \"expected_answer\": \"You must wear your seatbelt at all times during the ride. To use it, pull the belt across your body and insert the latch into the buckle until it clicks.\",\n",
        "    \"image_url\": [make_image_url(Path(\"seatbelt.png\")), make_image_url(Path(\"seatbelt.png\")), make_image_url(Path(\"seatbelt.png\"))]\n",
        "  },\n",
        "  {\n",
        "    \"question\": \"I think my seatbelt is missing\",\n",
        "    \"expected_answer\": \"The seatbelt should be located beside your seat, typically to the left or right. If you can't find it, check if it is retracted into the seat or behind you.\",\n",
        "    \"image_url\": make_image_url(Path(\"seatbelt.png\"))\n",
        "  },\n",
        "  {\n",
        "    \"question\": \"How do I open the door?\",\n",
        "    \"expected_answer\": \"To open the door, press the illuminated button with the door icon on the side panel. The door will unlock and open automatically.\",\n",
        "    \"image_url\": make_image_url(Path(\"open_door.png\"))\n",
        "  },\n",
        "  {\n",
        "    \"question\": \"How do I change the temperature?\",\n",
        "    \"expected_answer\": \"Use the on-screen temperature slider or touch controls near the air vents to adjust the cabin temperature. Slide left for cooler and right for warmer.\",\n",
        "    \"image_url\": make_image_url(Path(\"temperature.png\"))\n",
        "  },\n",
        "  {\n",
        "    \"question\": \"What is the coldest I can make the car?\",\n",
        "    \"expected_answer\": \"You can lower the temperature by sliding the temperature control all the way to the left until the blue or snowflake icon appears, indicating the coldest setting.\",\n",
        "    \"image_url\": make_image_url(Path(\"temperature.png\"))\n",
        "  },\n",
        "  {\n",
        "    \"question\": \"Am I able to drive the car?\",\n",
        "    \"expected_answer\": \"No, this vehicle is self-driving. All driving functions are handled by the system, and passengers cannot manually control the vehicle.\",\n",
        "    \"image_url\": make_image_url(Path(\"temperature.png\"))\n",
        "  },\n",
        "]"
      ],
      "metadata": {
        "id": "SMjfZ8e3sUhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have our dataset, we can create a dataset in Opik and insert the questions into it."
      ],
      "metadata": {
        "id": "01ZfKmPKuI9b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get or create a dataset\n",
        "client = opik.Opik()\n",
        "\n",
        "dataset = client.get_or_create_dataset(name=\"Zoox-FAQ\",\n",
        "                                       description=\"Zoox-FAQ\")\n",
        "\n",
        "# Inserting will not duplicate entries\n",
        "dataset.insert(dataset_items)"
      ],
      "metadata": {
        "id": "9ZgHuZvxuIdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating the Assistant\n",
        "\n",
        "In order to ensure our RAG application is working correctly and determine the system prompt to use in production, we will test it on our dataset with 3 different system prompts.\n",
        "\n",
        "For this we will be using the `evaluate` function from the `opik` library. We will evaluate the application on two metrics: Hallucination and AnswerRelevance.\n",
        "\n",
        "**Step 1: Fetch the dataset for evaluation**"
      ],
      "metadata": {
        "id": "hhPzHnXdue0I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client = opik.Opik()\n",
        "\n",
        "dataset = client.get_dataset(name=\"Zoox-FAQ\")"
      ],
      "metadata": {
        "id": "s6K4lEdpumWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Define the system prompt to test**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WMTjbCxkup30"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = opik.Prompt(\n",
        "    name=\"Zoox Chat Assistant\",\n",
        "    prompt=\"\"\"\n",
        "        You are a chat assistant. Answer questions to best of your ability.\n",
        "        \"\"\".rstrip().lstrip()\n",
        ")"
      ],
      "metadata": {
        "id": "1jTeIcWn0bIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: Define Evaluation Task**\n",
        "\n",
        "The evaluation task maps each input to the retrieved context and LLM output. These values will be used by Opik when calculating the metrics defined in the next step."
      ],
      "metadata": {
        "id": "HedDKrBLwAtA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluation_task(x):\n",
        "    full_response = bot.get_answer(x['question'], x['image_url'], system_prompt.format())\n",
        "    response = full_response[\"response\"]\n",
        "    context = full_response[\"context\"]\n",
        "    return {\n",
        "        \"response\": response,\n",
        "        \"context\": context\n",
        "    }"
      ],
      "metadata": {
        "id": "YiMwV6G5Hac7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4: Define Metrics**\n",
        "\n",
        "Defining a Custom Metric Using Image Inputs\n"
      ],
      "metadata": {
        "id": "C2KYE8QZonMF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from opik.evaluation.metrics import base_metric, score_result\n",
        "from openai import OpenAI\n",
        "from typing import Any\n",
        "import json\n",
        "import re\n",
        "\n",
        "class LLMJudgeMetric(base_metric.BaseMetric):\n",
        "    def __init__(self, name: str = \"Image Relevance\", model_name: str = \"gpt-4o\"):\n",
        "        self.name = name\n",
        "        self.llm_client = OpenAI()\n",
        "        self.model_name = model_name\n",
        "        self.prompt_template = \"\"\"\n",
        "        You are an impartial judge evaluating how accurately a response answers a question using a provided image as evidence.\n",
        "\n",
        "        Return only a JSON object with no explanation or extra text.\n",
        "\n",
        "        Score the response as:\n",
        "        - 1 if it is fully accurate based on the image\n",
        "        - 0.5 if partially accurate or uncertain\n",
        "        - 0 if inaccurate or not supported\n",
        "\n",
        "        It is very important that the format of your response should be a json with no backticks that returns:\n",
        "        {{\n",
        "            \"score\": <score between 0 and 1>,\n",
        "            \"reason\": \"<reason for the score>\"\n",
        "        }}\n",
        "\n",
        "        Question and Response:\n",
        "        {output}\n",
        "        \"\"\"\n",
        "\n",
        "    def score(self, output: str, image_url: dict, **ignored_kwargs: Any):\n",
        "        \"\"\"\n",
        "        Score the output of an LLM.\n",
        "        Args:\n",
        "            output: The output of an LLM to score.\n",
        "            **ignored_kwargs: Any additional keyword arguments. This is important so that the metric can be used in the `evaluate` function.\n",
        "        \"\"\"\n",
        "        # Construct the prompt based on the output of the LLM\n",
        "        prompt = self.prompt_template.format(output=output, image_url=image_url)\n",
        "        # Generate and parse the response from the LLM\n",
        "        response = self.llm_client.chat.completions.create(\n",
        "            model=self.model_name,\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": [\n",
        "                        {\n",
        "                            \"type\": \"text\",\n",
        "                            \"text\": prompt\n",
        "                        },\n",
        "                        {\n",
        "                            \"type\": \"image_url\",\n",
        "                            \"image_url\": image_url\n",
        "                        }\n",
        "                    ]\n",
        "                }\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        raw_output = response.choices[0].message.content.strip()\n",
        "\n",
        "        match = re.search(r'{\\s*\"score\"\\s*:\\s*(0(?:\\.5)?|1)(?:,\\s*\"reason\"\\s*:\\s*\".*?\")?\\s*}', raw_output, re.DOTALL)\n",
        "\n",
        "        if not match:\n",
        "            raise ValueError(f\"Could not extract JSON from model output:\\n{raw_output}\")\n",
        "\n",
        "        # Convert matched JSON string to dict\n",
        "        response_dict = json.loads(match.group(0))\n",
        "        response_score = float(response_dict[\"score\"])\n",
        "\n",
        "        return score_result.ScoreResult(\n",
        "            name=self.name,\n",
        "            value=response_score\n",
        "        )\n",
        "\n",
        "image_relevance = LLMJudgeMetric()\n"
      ],
      "metadata": {
        "id": "4EuS1Xo5lGcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we use Comet's built-in [Levenshtein Ratio](https://www.comet.com/docs/opik/evaluation/metrics/heuristic_metrics#levenshteinratio) and [AnswerRelevance](https://www.comet.com/docs/opik/evaluation/metrics/answer_relevance) metrics."
      ],
      "metadata": {
        "id": "btTRypJSRr8U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the metrics\n",
        "answerrelevance_metric = AnswerRelevance(name=\"AnswerRelevance\")\n",
        "levenshteinratio_metric = LevenshteinRatio(name=\"LevenshteinRatio\")"
      ],
      "metadata": {
        "id": "vHiW5RGHoukC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 5: Run the evaluation**\n",
        "\n",
        "Input the dataset, experiment config, evaluation task, and metrics into Opik's `evaluate` to run the evaluation."
      ],
      "metadata": {
        "id": "myI02oOTwExG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "experiment_config = {\"model\": \"gpt-4o\"}\n",
        "experiment_name = f\"zoox-faq-v1\"\n",
        "\n",
        "res = evaluate(\n",
        "    dataset=dataset,\n",
        "    experiment_name=experiment_name,\n",
        "    experiment_config=experiment_config,\n",
        "    project_name=PROJECT_NAME,\n",
        "    task=evaluation_task,\n",
        "    prompt=system_prompt,\n",
        "    scoring_metrics=[answerrelevance_metric, levenshteinratio_metric, image_relevance],\n",
        "    scoring_key_mapping={\n",
        "        \"input\": \"question\",\n",
        "        \"output\": \"response\",\n",
        "        \"reference\": \"expected_answer\"\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "Hv7zEC_AwGyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nMRbeDTbD_W"
      },
      "source": [
        "The evaluation results are now uploaded to the Opik platform and can be viewed in the UI."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating the Assistant (II)\n",
        "\n",
        "Prompt Engineering is an iterative process. Let's try a different system prompt."
      ],
      "metadata": {
        "id": "cbOXKd6sDaYs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = opik.Prompt(\n",
        "    name=\"Zoox Chat Assistant\",\n",
        "    prompt=\"\"\"\n",
        "        You are Zoey, a friendly, knowledgeable virtual assistant designed to help users have a safe and comfortable experience while riding in autonomous vehicles. Your primary role is to answer user questions clearly and accurately, drawing from a curated FAQ knowledge base and analyzing images provided by the user when relevant (e.g., photos of the vehicle interior, displays, or signage).\n",
        "\n",
        "        Goals:\n",
        "        Provide fast, reliable answers to questions about policies, safety, accessibility, vehicle features, and how to interact with the autonomous vehicle.\n",
        "\n",
        "        Offer guidance based on visual context when users submit images (e.g., interpreting screen messages, identifying seatbelt indicators, or showing how to use in-vehicle controls).\n",
        "\n",
        "        Always prioritize clarity, reassurance, and safety in your responses.\n",
        "        \"\"\".rstrip().lstrip()\n",
        ")"
      ],
      "metadata": {
        "id": "V8zbgPmp3Rem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "experiment_config = {\"model\": \"gpt-4o\"}\n",
        "experiment_name = f\"zoox-faq-v2\"\n",
        "\n",
        "res = evaluate(\n",
        "    dataset=dataset,\n",
        "    experiment_name=experiment_name,\n",
        "    experiment_config=experiment_config,\n",
        "    project_name=PROJECT_NAME,\n",
        "    task=evaluation_task,\n",
        "    prompt=system_prompt,\n",
        "    scoring_metrics=[answerrelevance_metric, levenshteinratio_metric, image_relevance],\n",
        "    scoring_key_mapping={\n",
        "        \"input\": \"question\",\n",
        "        \"output\": \"response\",\n",
        "        \"reference\": \"expected_answer\"\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "9pMJQGupE4gY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "py312_llm_eval",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}