{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VPUs5G8xw2XS"
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k6QlAJfEw4nu"
   },
   "source": [
    "[Catalyst](https://catalyst-team.github.io/catalyst/index.html) is a PyTorch framework for Deep Learning R&D. It focuses on reproducibility, rapid experimentation, and codebase reuse so you can create something new rather than write yet another train loop. The Catalyst library incorporates research best practices so users can focus on building models and not worry about writing boilerplate code.   \n",
    "\n",
    "[Comet](https://www.comet.com/site/data-scientists/?utm_campaign=gradio-integration&utm_medium=colab) is an MLOps Platform that is designed to help Data Scientists and Teams build better models faster! Comet provides tooling to track, Explain, Manage, and Monitor your models in a single place! It works with Jupyter Notebooks and Scripts and most importantely it's 100% free!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y4hki2b7sofT"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "01UyDh4SUXes"
   },
   "outputs": [],
   "source": [
    "%pip install catalyst \"comet_ml>=3.44.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z6mx-zruvxRB"
   },
   "source": [
    "# Login to Comet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O7g_JrgB64ob"
   },
   "outputs": [],
   "source": [
    "import comet_ml\n",
    "\n",
    "comet_ml.login(project_name=\"comet-example-catalyst-notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08NVENqt9hpZ"
   },
   "source": [
    "# Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4stX87FG4Pjd"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from catalyst import dl, utils\n",
    "from catalyst.data import ToTensor\n",
    "from catalyst.contrib.datasets import MNIST\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from catalyst import dl\n",
    "from catalyst.callbacks.checkpoint import CheckpointCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "74vfAVXHTGfI"
   },
   "source": [
    "# Logging to Comet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N1sevzQl4KKx"
   },
   "outputs": [],
   "source": [
    "def train(logger, hparams={\"lr\": 0.02, \"batch_size\": 32}):\n",
    "    model = nn.Sequential(nn.Flatten(), nn.Linear(28 * 28, 10))\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=hparams[\"lr\"])\n",
    "\n",
    "    loaders = {\n",
    "        \"train\": DataLoader(\n",
    "            MNIST(os.getcwd(), train=True, download=True, transform=ToTensor()),\n",
    "            batch_size=hparams[\"batch_size\"],\n",
    "        ),\n",
    "        \"valid\": DataLoader(\n",
    "            MNIST(os.getcwd(), train=False, download=True, transform=ToTensor()),\n",
    "            batch_size=hparams[\"batch_size\"],\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    runner = dl.SupervisedRunner(\n",
    "        input_key=\"features\", output_key=\"logits\", target_key=\"targets\", loss_key=\"loss\"\n",
    "    )\n",
    "\n",
    "    # model training\n",
    "    runner.train(\n",
    "        model=model,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        loaders=loaders,\n",
    "        hparams=hparams,\n",
    "        num_epochs=1,\n",
    "        callbacks=[\n",
    "            dl.AccuracyCallback(\n",
    "                input_key=\"logits\", target_key=\"targets\", topk_args=(1, 3, 5)\n",
    "            ),\n",
    "            dl.PrecisionRecallF1SupportCallback(\n",
    "                input_key=\"logits\", target_key=\"targets\", num_classes=10\n",
    "            ),\n",
    "        ],\n",
    "        logdir=\"./logs\",\n",
    "        valid_loader=\"valid\",\n",
    "        valid_metric=\"loss\",\n",
    "        minimize_valid_metric=True,\n",
    "        verbose=True,\n",
    "        load_best_on_end=True,\n",
    "        loggers={\"comet\": logger},\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kFr6EbdS4FP5"
   },
   "source": [
    "## Logging your training run to a Comet Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zm2ZNi174ClH"
   },
   "outputs": [],
   "source": [
    "logger = dl.CometLogger()\n",
    "train(logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cpIzT5kz90-A"
   },
   "source": [
    "### Visualize your training Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SmSpe2p4-ra8"
   },
   "source": [
    "Visualize your metrics without leaving the notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PNL9zSjz9wkb"
   },
   "outputs": [],
   "source": [
    "logger.experiment.display(tab=\"charts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SMpeRu0i9d8m"
   },
   "source": [
    "## Logging a training run while Running Offline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OWCpAU0CqCG4"
   },
   "source": [
    "There may be situations where you would like to log your metrics, parameters, source code, etc, but you might not be able to access the public internet. (e.g. Running on a cluster inside a private network).\n",
    "\n",
    "In those situations, you can run Comet's logging in Offline Mode. This will log your run as a zip file that you can upload later to the Comet UI.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_k2bW7Zd9fow"
   },
   "outputs": [],
   "source": [
    "logger = dl.CometLogger(comet_mode=\"offline\", **{\"offline_directory\": \"/tmp\"})\n",
    "train(logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "81Co00LA84_N"
   },
   "outputs": [],
   "source": [
    "! comet upload /tmp/<your-experiment-id>.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HvATF_a7HC6m"
   },
   "source": [
    "## Continuing an existing Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WIOsAoHKrf0G"
   },
   "source": [
    "In order to resume a training run from where you left off, you will have to provide the experiment id of the existing run to the Comet logger.\n",
    "\n",
    "Additionally, you can pass a list of strings to tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WGQ3GP7mHMOt"
   },
   "outputs": [],
   "source": [
    "previous_experiment_id = \"previous-experiment-id\"\n",
    "logger = dl.CometLogger(experiment_id=previous_experiment_id, tags=[\"resumed\"])\n",
    "train(logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XG4KV4pgqRJY"
   },
   "source": [
    "## Logging Multi Stage Runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bGTpYFZcvKAl"
   },
   "source": [
    "One of the key benefits of Catalyst is the ability to create multi-stage runs. The Comet Logger supports this right out of the box. Each metric and parameter in a multi-stage run will be logged to comet with a prefix denoting the stage.\n",
    "\n",
    "The metric name format is `{stage_key}/{loader_key}_{metric_name}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r7_8JXnP7k0q"
   },
   "outputs": [],
   "source": [
    "class CustomRunner(dl.IRunner):\n",
    "    def __init__(self, logdir, device):\n",
    "        # you could add all required extra params during Runner initialization\n",
    "        # for our case, let's customize ``logdir`` and ``engine`` for the runs\n",
    "        super().__init__()\n",
    "        self._logdir = logdir\n",
    "        self._device = device\n",
    "\n",
    "    def get_engine(self):\n",
    "        return dl.DeviceEngine(self._device)\n",
    "\n",
    "    def get_loggers(self):\n",
    "        return {\n",
    "            \"comet\": dl.CometLogger(),\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def stages(self):\n",
    "        # suppose we have 2 stages:\n",
    "        # 1st - with freezed encoder\n",
    "        # 2nd with unfreezed whole network\n",
    "        return [\"train_freezed\", \"train_unfreezed\"]\n",
    "\n",
    "    def get_stage_len(self, stage: str) -> int:\n",
    "        return 3\n",
    "\n",
    "    def get_loaders(self, stage: str):\n",
    "        loaders = {\n",
    "            \"train\": DataLoader(\n",
    "                MNIST(os.getcwd(), train=True, download=True, transform=ToTensor()),\n",
    "                batch_size=32,\n",
    "            ),\n",
    "            \"valid\": DataLoader(\n",
    "                MNIST(os.getcwd(), train=False, download=True, transform=ToTensor()),\n",
    "                batch_size=32,\n",
    "            ),\n",
    "        }\n",
    "        return loaders\n",
    "\n",
    "    def get_model(self, stage: str):\n",
    "        # the logic here is quite straightforward:\n",
    "        # we create the model on the fist stage\n",
    "        # and reuse it during next stages\n",
    "        model = (\n",
    "            self.model\n",
    "            if self.model is not None\n",
    "            else nn.Sequential(\n",
    "                nn.Flatten(), nn.Linear(784, 128), nn.ReLU(), nn.Linear(128, 10)\n",
    "            )\n",
    "        )\n",
    "        if stage == \"train_freezed\":\n",
    "            # 1st stage\n",
    "            # freeze layer\n",
    "            utils.set_requires_grad(model[1], False)\n",
    "        else:\n",
    "            # 2nd stage\n",
    "            utils.set_requires_grad(model, True)\n",
    "        return model\n",
    "\n",
    "    def get_criterion(self, stage: str):\n",
    "        return nn.CrossEntropyLoss()\n",
    "\n",
    "    def get_optimizer(self, stage: str, model):\n",
    "        # we could also define different components for the different stages\n",
    "        if stage == \"train_freezed\":\n",
    "            return optim.Adam(model.parameters(), lr=1e-3)\n",
    "        else:\n",
    "            return optim.SGD(model.parameters(), lr=1e-1)\n",
    "\n",
    "    def get_scheduler(self, stage: str, optimizer):\n",
    "        return None\n",
    "\n",
    "    def get_callbacks(self, stage: str):\n",
    "        return {\n",
    "            \"criterion\": dl.CriterionCallback(\n",
    "                metric_key=\"loss\", input_key=\"logits\", target_key=\"targets\"\n",
    "            ),\n",
    "            \"optimizer\": dl.OptimizerCallback(metric_key=\"loss\"),\n",
    "            # \"scheduler\": dl.SchedulerCallback(loader_key=\"valid\", metric_key=\"loss\"),\n",
    "            \"accuracy\": dl.AccuracyCallback(\n",
    "                input_key=\"logits\", target_key=\"targets\", topk_args=(1, 3, 5)\n",
    "            ),\n",
    "            \"classification\": dl.PrecisionRecallF1SupportCallback(\n",
    "                input_key=\"logits\", target_key=\"targets\", num_classes=10\n",
    "            ),\n",
    "            # catalyst[ml] required\n",
    "            # \"confusion_matrix\": dl.ConfusionMatrixCallback(\n",
    "            #     input_key=\"logits\", target_key=\"targets\", num_classes=10\n",
    "            # ),\n",
    "            \"checkpoint\": dl.CheckpointCallback(\n",
    "                self._logdir,\n",
    "                loader_key=\"valid\",\n",
    "                metric_key=\"loss\",\n",
    "                minimize=True,\n",
    "                save_n_best=3,\n",
    "            ),\n",
    "        }\n",
    "\n",
    "    def handle_batch(self, batch):\n",
    "        x, y = batch\n",
    "        logits = self.model(x)\n",
    "\n",
    "        self.batch = {\n",
    "            \"features\": x,\n",
    "            \"targets\": y,\n",
    "            \"logits\": logits,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-kaV527kCIe4"
   },
   "outputs": [],
   "source": [
    "runner = CustomRunner(\"/tmp\", \"cuda\")\n",
    "runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OmHkKRd6FqzI"
   },
   "source": [
    "## Logging Model Checkpoints and Arbitary Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3IqhXtPdwJKu"
   },
   "source": [
    "The CometLogger also supports logging arbitary data files. In this example, we subclass the Checkpoint Callback and use the CometLogger to log our model weights\n",
    "\n",
    "We can use a similar callback structure to log any arbitary data to Comet such as files containing model predictions, audio, text etc.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_EdVhgQXFihE"
   },
   "outputs": [],
   "source": [
    "logger = dl.CometLogger()\n",
    "\n",
    "# sample data\n",
    "num_samples, num_features, num_classes = int(1e4), int(1e1), 4\n",
    "X = torch.rand(num_samples, num_features)\n",
    "y = (\n",
    "    torch.rand(\n",
    "        num_samples,\n",
    "    )\n",
    "    * num_classes\n",
    ").to(torch.int64)\n",
    "\n",
    "# pytorch loaders\n",
    "dataset = TensorDataset(X, y)\n",
    "loader = DataLoader(dataset, batch_size=32, num_workers=1)\n",
    "loaders = {\"train\": loader, \"valid\": loader}\n",
    "\n",
    "# model, criterion, optimizer, scheduler\n",
    "model = torch.nn.Linear(num_features, num_classes)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [2])\n",
    "\n",
    "\n",
    "class CometCheckpointCallback(CheckpointCallback):\n",
    "    def __init__(self, logdir, logger, best_score=None, save_n_best=1):\n",
    "        super().__init__(logdir)\n",
    "        self.logdir = logdir\n",
    "        self.logger = logger\n",
    "        self.save_n_best = save_n_best\n",
    "\n",
    "    def on_epoch_end(self, runner: \"IRunner\") -> None:\n",
    "        \"\"\"\n",
    "        Collects and saves checkpoint after epoch.\n",
    "\n",
    "        Args:\n",
    "            runner: current runner\n",
    "        \"\"\"\n",
    "        if runner.is_infer_stage:\n",
    "            return\n",
    "        if runner.engine.is_ddp and not runner.engine.is_master_process:\n",
    "            return\n",
    "\n",
    "        if self._use_model_selection:\n",
    "            # score model based on the specified metric\n",
    "            score = runner.epoch_metrics[self.loader_key][self.metric_key]\n",
    "        else:\n",
    "            # score model based on epoch number\n",
    "            score = runner.global_epoch_step\n",
    "\n",
    "        is_best = False\n",
    "        if self.best_score is None or self.is_better(score, self.best_score):\n",
    "            self.best_score = score\n",
    "            is_best = True\n",
    "\n",
    "        if self.save_n_best > 0:\n",
    "            # pack checkpoint\n",
    "            checkpoint = self._pack_checkpoint(runner)\n",
    "            # save checkpoint\n",
    "            checkpoint_path = self._save_checkpoint(\n",
    "                runner=runner,\n",
    "                checkpoint=checkpoint,\n",
    "                is_best=is_best,\n",
    "                is_last=True,\n",
    "            )\n",
    "            self.logger.log_artifact(\n",
    "                path_to_artifact=checkpoint_path,\n",
    "                global_batch_step=runner.global_batch_step,\n",
    "            )\n",
    "\n",
    "\n",
    "# model training\n",
    "runner = dl.SupervisedRunner(\n",
    "    input_key=\"features\", output_key=\"logits\", target_key=\"targets\", loss_key=\"loss\"\n",
    ")\n",
    "runner.train(\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    loaders=loaders,\n",
    "    logdir=\"./logdir\",\n",
    "    num_epochs=3,\n",
    "    valid_loader=\"valid\",\n",
    "    valid_metric=\"accuracy03\",\n",
    "    minimize_valid_metric=False,\n",
    "    verbose=True,\n",
    "    callbacks=[\n",
    "        dl.AccuracyCallback(\n",
    "            input_key=\"logits\", target_key=\"targets\", num_classes=num_classes\n",
    "        ),\n",
    "        CometCheckpointCallback(logdir=\"/tmp\", logger=logger),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bd-E-S_B-4ua"
   },
   "source": [
    "### Let's take a look at the Logged Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fjCdbHVW-_Nu"
   },
   "outputs": [],
   "source": [
    "logger.experiment.display(tab=\"assets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NLSTBUcyyQH1"
   },
   "source": [
    "## Logging Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nO8pPnpsn2Gb"
   },
   "outputs": [],
   "source": [
    "logger = dl.CometLogger()\n",
    "\n",
    "# sample data\n",
    "num_samples, num_features, num_classes = int(1e4), int(1e1), 4\n",
    "X = torch.rand(num_samples, num_features)\n",
    "y = (\n",
    "    torch.rand(\n",
    "        num_samples,\n",
    "    )\n",
    "    * num_classes\n",
    ").to(torch.int64)\n",
    "\n",
    "# pytorch loaders\n",
    "dataset = TensorDataset(X, y)\n",
    "loader = DataLoader(dataset, batch_size=32, num_workers=1)\n",
    "loaders = {\"train\": loader, \"valid\": loader}\n",
    "\n",
    "# model, criterion, optimizer, scheduler\n",
    "model = torch.nn.Linear(num_features, num_classes)\n",
    "\n",
    "# model training\n",
    "runner = dl.SupervisedRunner(\n",
    "    input_key=\"features\", output_key=\"logits\", target_key=\"targets\", loss_key=\"loss\"\n",
    ")\n",
    "metrics = runner.evaluate_loader(\n",
    "    loader,\n",
    "    callbacks=[\n",
    "        dl.AccuracyCallback(\n",
    "            input_key=\"logits\", target_key=\"targets\", num_classes=num_classes\n",
    "        ),\n",
    "    ],\n",
    "    model=model,\n",
    ")\n",
    "logger.log_metrics(metrics, stage_key=\"evaluation\", loader_key=\"valid\", scope=\"batch\")\n",
    "logger.experiment.end()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
