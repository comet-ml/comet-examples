{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is meant to be run in Sagemaker Notebook Instance. We will train a Transformer Model on the IMDB dataset while logging data to Comet in real time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Comet and other Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -U \"comet_ml>=3.44.0\" \"sagemaker>=2.140.0\" \"transformers==4.26.1\" \"datasets[s3]\" \"boto3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Login to Comet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import comet_ml\n",
    "\n",
    "COMET_PROJECT_NAME = \"comet-example-sagemaker-custom-transformers-text-classification\"\n",
    "\n",
    "comet_ml.login(project_name=COMET_PROJECT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch Sagemaker Credentials "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "prefix = \"sagemaker/DEMO-huggingface-imdb\"\n",
    "bucket = sess.default_bucket()\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Fetch the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# tokenizer used in preprocessing\n",
    "tokenizer_name = \"distilbert-base-uncased\"\n",
    "\n",
    "# dataset used\n",
    "dataset_name = \"imdb\"\n",
    "\n",
    "# download tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "# tokenizer helper function\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "# load dataset\n",
    "train_dataset, test_dataset = load_dataset(\"imdb\", split=[\"train\", \"test\"])\n",
    "\n",
    "train_dataset = test_dataset.shuffle().select(range(1000))\n",
    "test_dataset = test_dataset.shuffle().select(range(100))\n",
    "\n",
    "# tokenize dataset\n",
    "train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize, batched=True)\n",
    "\n",
    "# set format for pytorch\n",
    "train_dataset = train_dataset.rename_column(\"label\", \"labels\")\n",
    "train_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "test_dataset = test_dataset.rename_column(\"label\", \"labels\")\n",
    "test_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "# save train_dataset to s3\n",
    "training_input_path = f\"s3://{sess.default_bucket()}/{prefix}/train\"\n",
    "train_dataset.save_to_disk(training_input_path)\n",
    "\n",
    "# save test_dataset to s3\n",
    "test_input_path = f\"s3://{sess.default_bucket()}/{prefix}/test\"\n",
    "test_dataset.save_to_disk(test_input_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "AWS_INSTANCE_TYPE = \"ml.g4dn.xlarge\"\n",
    "AWS_INSTANCE_COUNT = 1\n",
    "\n",
    "HYPERPARAMETERS = {\n",
    "    \"epochs\": 1,\n",
    "    \"train_batch_size\": 32,\n",
    "    \"log-interval\": 1,\n",
    "    \"model_name\": \"distilbert-base-uncased\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the Sagemaker Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "COMET_API_KEY = comet_ml.config.get_config()[\"comet.api_key\"]\n",
    "COMET_PROJECT_NAME = comet_ml.config.get_config()[\"comet.project_name\"]\n",
    "\n",
    "estimator = HuggingFace(\n",
    "    source_dir=\"src\",\n",
    "    entry_point=\"text_classification.py\",\n",
    "    role=role,\n",
    "    transformers_version=\"4.26\",\n",
    "    pytorch_version=\"1.13\",\n",
    "    py_version=\"py39\",\n",
    "    instance_count=AWS_INSTANCE_COUNT,\n",
    "    instance_type=AWS_INSTANCE_TYPE,\n",
    "    hyperparameters=HYPERPARAMETERS,\n",
    "    environment={\n",
    "        \"COMET_API_KEY\": COMET_API_KEY,\n",
    "        \"COMET_PROJECT_NAME\": COMET_PROJECT_NAME,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "estimator.fit({\"train\": training_input_path, \"test\": test_input_path})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_pytorch_latest_p37",
   "language": "python",
   "name": "conda_amazonei_pytorch_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
