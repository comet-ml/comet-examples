{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQDordYwNFl4"
      },
      "source": [
        "<img src=\"https://cdn.comet.ml/img/notebook_logo.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIi6_8ecOQ3o"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "[Gradio](https://www.gradio.app/) allows you to quickly create customizable UI components around your TensorFlow or PyTorch models, or even arbitrary Python functions. Mix and match components to support any combination of inputs and outputs. Our core library is free and open-source!\n",
        "\n",
        "[Comet](https://www.comet.ml/site/data-scientists/?utm_campaign=gradio-integration&utm_medium=colab) is an MLOps Platform that is designed to help Data Scientists and Teams build better models faster! Comet provides tooling to track, Explain, Manage, and Monitor your models in a single place! It works with Jupyter Notebooks and Scripts and most importantly it's 100% free!\n",
        "\n",
        "In this notebook, we will teach you how to build a model evaluation application that lets you log SHAP inferences from a Text Generation model to Comet.          "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKno1ef4_lZL"
      },
      "source": [
        "# Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-Dq42vt_Vbr"
      },
      "outputs": [],
      "source": [
        "%pip install comet_ml torch transformers gradio shap --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kMzU2I2ChdB"
      },
      "source": [
        "# Initialize Comet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Pwx9aaICjSo",
        "outputId": "526d9261-f75d-46e2-d217-9dcdbc97925d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please enter your Comet API key from https://www.comet.com/api/my/settings/\n",
            "(api key may not show as you type)\n",
            "Comet API key: ··········\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "COMET INFO: Comet API key is valid\n",
            "COMET WARNING: running in Google Colab, but can't find mounted drive. Using HOME instead\n",
            "COMET WARNING: if drive is mounted, set COMET_CONFIG to save config there\n",
            "COMET INFO: Comet API key saved in /root/.comet.config\n"
          ]
        }
      ],
      "source": [
        "import comet_ml\n",
        "comet_ml.init(project_name='gradio-inference')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qL4YaWakC5qO"
      },
      "source": [
        "# Define Gradio UI\n",
        "\n",
        "We will build a simple UI to run a Text Generation model and log the SHAP inference plots to Comet as HTML files. \n",
        "\n",
        "We will leverage Gradio's `State` component to store our Experiment object, which allows us to log multiple inferences to a single Experiment.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "KO_UOTXJBmtg"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import shap\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "\n",
        "MODEL_NAME = \"gpt2\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# set model decoder to true\n",
        "model.config.is_decoder = True\n",
        "# set text-generation params under task_specific_params\n",
        "model.config.task_specific_params[\"text-generation\"] = {\n",
        "    \"do_sample\": True,\n",
        "    \"max_length\": 50,\n",
        "    \"temperature\": 0.7,\n",
        "    \"top_k\": 50,\n",
        "    \"no_repeat_ngram_size\": 2\n",
        "}\n",
        "model = model.to(device)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "explainer = shap.Explainer(model, tokenizer)\n",
        "\n",
        "\n",
        "def start_experiment():\n",
        "    \"\"\"Returns an APIExperiment object that is thread safe\n",
        "    and can be used to log inferences to a single Experiment\n",
        "    \"\"\"\n",
        "    try:\n",
        "        api = comet_ml.API()\n",
        "        workspace = api.get_default_workspace()\n",
        "        project_name = comet_ml.config.get_config()[\"comet.project_name\"]\n",
        "\n",
        "        experiment = comet_ml.APIExperiment(\n",
        "            workspace=workspace, project_name=project_name\n",
        "        )\n",
        "        experiment.log_other(\"Created from\", \"gradio-inference\")\n",
        "        return (experiment, f\"Started Experiment: {experiment.name}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        return None, None\n",
        "\n",
        "\n",
        "def predict(text, state, message):\n",
        "    experiment = state\n",
        "\n",
        "    shap_values = explainer([text])\n",
        "    plot = shap.plots.text(shap_values, display=False)\n",
        "\n",
        "    if experiment is not None:\n",
        "        experiment.log_other(\"message\", message)\n",
        "        experiment.log_html(plot)\n",
        "\n",
        "    return plot\n",
        "\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    start_experiment_btn = gr.Button(\"Start New Experiment\")\n",
        "    experiment_status = gr.Markdown()\n",
        "\n",
        "    # Log a message to the Experiment to provide more context\n",
        "    experiment_message = gr.Textbox(label=\"Experiment Message\")\n",
        "    experiment = gr.State()\n",
        "\n",
        "    input_text = gr.Textbox(label=\"Input Text\", lines=5, interactive=True)\n",
        "    submit_btn = gr.Button(\"Submit\")\n",
        "\n",
        "    output = gr.HTML(interactive=True)\n",
        "\n",
        "    start_experiment_btn.click(\n",
        "        start_experiment, outputs=[experiment, experiment_status]\n",
        "    )\n",
        "    submit_btn.click(\n",
        "        predict, inputs=[input_text, experiment, experiment_message], outputs=[output]\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vbYRxKBriox"
      },
      "source": [
        "# Start the Gradio App "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g10qxXwD2MCV"
      },
      "outputs": [],
      "source": [
        "demo.launch(share=True, debug=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}