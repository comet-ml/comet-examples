{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PWVljpddz_vN"
   },
   "source": [
    "<img src=\"https://cdn.comet.ml/img/notebook_logo.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A0-thQauBRRL"
   },
   "source": [
    "[Comet](https://www.comet.com/site/products/ml-experiment-tracking/?utm_campaign=pytorch&utm_medium=colab) is an MLOps Platform that is designed to help Data Scientists and Teams build better models faster! Comet provides tooling to track, Explain, Manage, and Monitor your models in a single place! It works with Jupyter Notebooks and Scripts and most importantly it's 100% free to get started!\n",
    "\n",
    "[PyTorch](https://pytorch.org/) is a popular open source machine learning framework based on the Torch library, used for applications such as computer vision and natural language processing.\n",
    "\n",
    "PyTorch enables fast, flexible experimentation and efficient production through a user-friendly front-end, distributed training, and ecosystem of tools and libraries.\n",
    "\n",
    "Instrument PyTorch with Comet to start managing experiments, create dataset versions and track hyperparameters for faster and easier reproducibility and collaboration.\n",
    "\n",
    "[Find more information about our integration with Pytorch](https://www.comet.ml/docs/v2/integrations/ml-frameworks/pytorch/)\n",
    "\n",
    "Curious about how Comet can help you build better models, faster? Find out more about [Comet](https://www.comet.com/site/products/ml-experiment-tracking/?utm_campaign=pytorch&utm_medium=colab) and our [other integrations](https://www.comet.ml/docs/v2/integrations/overview/)\n",
    "\n",
    "Get a preview for what's to come. Check out a completed experiment created from this notebook [here](https://www.comet.com/examples/comet-example-pytorch-histogram-logging/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V2UZtdWitSLf"
   },
   "source": [
    "# Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vIQsPNvatQIU",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "%pip install comet_ml torch torchvision tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lpCFdN33tday"
   },
   "source": [
    "# Initialize Comet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6M04rI0TOhdS",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import comet_ml\n",
    "\n",
    "comet_ml.init(project_name=\"comet-example-pytorch-histogram-logging\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zMVITKxktj7H"
   },
   "source": [
    "# Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZKGQZqjd8as8",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim\n",
    "\n",
    "from collections import OrderedDict\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Comet Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vRrV360UADWZ",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "experiment = comet_ml.Experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_params = {\"batch_size\": 100, \"num_epochs\": 3, \"learning_rate\": 0.01}\n",
    "experiment.log_parameters(hyper_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6A3r-jeYiTva",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]\n",
    ")\n",
    "\n",
    "trainset = datasets.MNIST(\"/tmp\", download=True, train=True, transform=transform)\n",
    "valset = datasets.MNIST(\"/tmp\", download=True, train=False, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=hyper_params[\"batch_size\"], shuffle=True\n",
    ")\n",
    "valloader = torch.utils.data.DataLoader(\n",
    "    valset, batch_size=hyper_params[\"batch_size\"], shuffle=True\n",
    ")\n",
    "\n",
    "input_size = 784\n",
    "hidden_sizes = [128, 64]\n",
    "output_size = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SSA9r_AziXbh",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    OrderedDict(\n",
    "        [\n",
    "            (\"linear0\", nn.Linear(input_size, hidden_sizes[0])),\n",
    "            (\"activ0\", nn.ReLU()),\n",
    "            (\"linear1\", nn.Linear(hidden_sizes[0], hidden_sizes[1])),\n",
    "            (\"activ1\", nn.ReLU()),\n",
    "            (\"linear2\", nn.Linear(hidden_sizes[1], output_size)),\n",
    "            (\"output\", nn.LogSoftmax(dim=1)),\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "optimizer = optim.Adam(model.parameters(), lr=hyper_params[\"learning_rate\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define helper functions to logs gradients and weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BqQfFK4_8iYE",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def to_numpy(x):\n",
    "    return x.detach().numpy()\n",
    "\n",
    "\n",
    "def update_gradient_map(model, gradmap):\n",
    "    for name, layer in zip(model._modules, model.children()):\n",
    "        if \"activ\" in name:\n",
    "            continue\n",
    "\n",
    "        if not hasattr(layer, \"weight\"):\n",
    "            continue\n",
    "\n",
    "        wname = \"%s/%s.%s\" % (\"gradient\", name, \"weight\")\n",
    "        bname = \"%s/%s.%s\" % (\"gradient\", name, \"bias\")\n",
    "\n",
    "        gradmap.setdefault(wname, 0)\n",
    "        gradmap.setdefault(bname, 0)\n",
    "\n",
    "        gradmap[wname] += layer.weight.grad\n",
    "        gradmap[bname] += layer.bias.grad\n",
    "\n",
    "    return gradmap\n",
    "\n",
    "\n",
    "def log_gradients(gradmap, step):\n",
    "    for k, v in gradmap.items():\n",
    "        experiment.log_histogram_3d(to_numpy(v), name=k, step=step)\n",
    "\n",
    "\n",
    "def log_weights(model, step):\n",
    "    for name, layer in zip(model._modules, model.children()):\n",
    "        if \"activ\" in name:\n",
    "            continue\n",
    "\n",
    "        if not hasattr(layer, \"weight\"):\n",
    "            continue\n",
    "\n",
    "        wname = \"%s.%s\" % (name, \"weight\")\n",
    "        bname = \"%s.%s\" % (name, \"bias\")\n",
    "\n",
    "        experiment.log_histogram_3d(to_numpy(layer.weight), name=wname, step=step)\n",
    "        experiment.log_histogram_3d(to_numpy(layer.bias), name=bname, step=step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, dataloader, epoch):\n",
    "    gradmap = {}\n",
    "    steps_per_epoch = len(dataloader.dataset) // hyper_params[\"batch_size\"]\n",
    "    total_loss = 0\n",
    "\n",
    "    with experiment.train():\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in tqdm(enumerate(dataloader)):\n",
    "            data = data.view(data.size(0), -1)\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(data)\n",
    "\n",
    "            loss = F.nll_loss(pred, target)\n",
    "            loss.backward()\n",
    "\n",
    "            gradmap = update_gradient_map(model, gradmap)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # scale gradients\n",
    "        for k, v in gradmap.items():\n",
    "            gradmap[k] = v / steps_per_epoch\n",
    "\n",
    "        log_gradients(gradmap, epoch * steps_per_epoch)\n",
    "        log_weights(model, epoch * steps_per_epoch)\n",
    "        total_loss /= len(dataloader.dataset)\n",
    "\n",
    "    experiment.log_metric(\"loss\", total_loss, epoch=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TjrfsK3-RfOF",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "max_epochs = hyper_params[\"num_epochs\"]\n",
    "for epoch in range(max_epochs + 1):\n",
    "    train(model, trainloader, epoch)\n",
    "\n",
    "experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
